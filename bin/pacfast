#!/bin/sh

sync="0"
packages=""
args=""

# rather naive approach to argument processing that i'm sure explodes in
# some combination of syncing with clean/refresh flags
while true; do
  case "$1" in
    -S | --sync) sync="1"; args="-S"; shift ;;
    --) shift; packages="$@"; break ;;
    *) args="$args $1"; shift ;;
  esac
done


# pass through all non-sync operations directly to pacman and exit
if [ "$sync" = "0" ]; then
  pacman "$args" "$packages" && exit 0
fi

# install aria2 if it's unavailable
if [ ! -x "$(command -v aria2c)" ]; then
  pacman -S --noconfirm aria2
fi

cache_dir="$(pacman-conf Cachedir)"
dl_list_file="${cache_dir}pacfast.file"

download_packages() {
  aria2c \
    --min-split-size="1M" \
    --max-connection-per-server="16" \
    --max-concurrent-downloads="16" \
    --split="16" \
    --timeout="10" \
    --connect-timeout="10" \
    --file-allocation="none" \
    --async-dns-server="8.8.8.8" \
    --continue="true" \
    --input-file="$dl_list_file" \
    --dir="$cache_dir"

  rm "$dl_list_file"
}   

# if we have unfinished jobs from a previous run, complete them
if [ -f "$dl_list_file" ]; then
  download_packages || exit 1
fi

# bail out early if we've been passed no packages to download
if [ -z "$packages" ]; then
  pacman $args

  exit 0
fi

# grab a list of our packages, only keep http(s) and ftp URIs, and create a
# download list for our download manager
pacman $args --print --print-format "%l" $packages | grep -Eoe "(ht|f)tp[s]?://[^\']+" | tee "$dl_list_file"

download_packages || exit 1

# strip out any refresh flags which would've been "ran" previously when we
# dumped the package URLs with --print. this avoids 'wasted' downloads on the
# chance a package was updated during downloading and would return new data
# on this subsequent call, but means there's a tiny chance we may receive an
# outdated version
pacman $(echo $args | sed 's/-y //g') $packages

